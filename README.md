# Alocador de Objetos com Aprendizagem por Refor√ßo (Q-Learning)

![Python](https://img.shields.io/badge/Python-3.8%2B-blue)
![MASPY](https://img.shields.io/badge/Library-MASPY-green)
![Status](https://img.shields.io/badge/Status-Conclu√≠do-success)

Este reposit√≥rio cont√©m a implementa√ß√£o de um projeto que consiste em um agente inteligente que utiliza **Aprendizagem por Refor√ßo (Q-Learning)** para aprender a alocar objetos em caixas espec√≠ficas, maximizando a recompensa acumulada.

## üìã Descri√ß√£o do Cen√°rio

O ambiente simula um rob√¥ alocador que deve mover **2 objetos** (inicialmente em uma prateleira) para uma de **3 caixas** dispon√≠veis (Caixa 1, Caixa 2 ou Caixa 3)[cite: 11, 12].

O objetivo do agente √© descobrir, atrav√©s de tentativa e erro (treinamento), qual caixa oferece a maior recompensa para cada objeto.

### Tabela de Recompensas

[cite_start]As regras de pontua√ß√£o definidas para o ambiente s√£o:

| A√ß√£o (Alocar) | Destino: Caixa 1 | Destino: Caixa 2 | Destino: Caixa 3 |
| :--- | :---: | :---: | :---: |
| **Objeto 1** | +5 | -5 | **+7 (√ìtimo)** |
| **Objeto 2** | -5 | **+5 (√ìtimo)** | -2 |

O comportamento √≥timo esperado √© que o agente aprenda a colocar o **Objeto 1 na Caixa 3** e o **Objeto 2 na Caixa 2**, totalizando uma recompensa de **12**.

## üß† Metodologia SART

A modelagem do problema seguiu a metodologia SART (*States, Actions, Rewards, Transitions*), conforme detalhado no arquivo `AC_Respostas.md`:

* **Estados:** 16 estados poss√≠veis (combina√ß√£o das 4 posi√ß√µes poss√≠veis para cada um dos 2 objetos).
* **A√ß√µes:** 6 a√ß√µes de movimento (mover Objeto 1 ou 2 para C1, C2 ou C3).
* **Algoritmo:** Q-Learning (implementado via biblioteca `maspy`).
* **Treinamento:** 15.000 epis√≥dios para garantir a converg√™ncia da tabela Q.

## üöÄ Como Executar

### Pr√©-requisitos
* Python 3.x instalado.
* Biblioteca `maspy` (certifique-se de que a pasta `maspy` esteja no diret√≥rio raiz ou instalada no ambiente).

### Sa√≠da Esperada
O script executar√° duas fases:
1.  **Fase de Treinamento:** O agente explora o ambiente por 15.000 epis√≥dios para preencher a Q-Table.
2.  **Fase de Execu√ß√£o (Infer√™ncia):** O agente utiliza a pol√≠tica aprendida ("c√©rebro") para realizar a aloca√ß√£o √≥tima em tempo real.

Exemplo de log no terminal:
```text
============================================================
DATA E HORA DA EXECU√á√ÉO: DD/MM/AAAA HH:MM:SS
ALUNO: Geraldo e Rom√°rio
============================================================

>>> INICIANDO TREINAMENTO (Q-LEARNING)...
>>> TREINAMENTO CONCLU√çDO.

>>> EXECUTANDO O AGENTE TREINADO (Modo Infer√™ncia)...
Robo_Alocador: Mover Objeto 2 -> CX 2 | Recompensa: 5 | (Decis√£o √ìtima? SIM)
Robo_Alocador: Mover Objeto 1 -> CX 3 | Recompensa: 7 | (Decis√£o √ìtima? SIM)

```

## üìù Metodologia SART

A metodologia SART (States, Actions, Rewards, Transitions) foi utilizada para modelar o problema:

* **States (Estados):** 16 combina√ß√µes poss√≠veis de localiza√ß√£o dos dois objetos (Prateleira, C1, C2, C3).
* **Actions (A√ß√µes):** 6 movimentos poss√≠veis (Mover Obj1 ou Obj2 para uma das 3 caixas).
* **Rewards (Recompensas):** Valores atribu√≠dos conforme a tabela de pontua√ß√£o, variando de -5 a +7.
* **Transitions (Transi√ß√µes):** L√≥gica determin√≠stica que atualiza o estado do ambiente ap√≥s cada a√ß√£o.

Para a documenta√ß√£o completa e detalhada, consulte o arquivo [AC_Respostas.md](./AC_Respostas.md).
